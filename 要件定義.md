# Learned-Index Citations – GitHub Pages App Scaffold

本リポジトリは、**“The Case for Learned Index Structures”**（以下 RMI）を**引用している全ての論文**を OpenAlex API から定期取得し、GitHub Pages 上で検索・可視化するための最小構成テンプレートです。自動タグ付け、著者/年度/会議・ジャーナル別の統計、被引用数の集計を含みます。GitHub Actions のスケジュール実行で毎日更新します。

---

## 1) リポジトリ構成

```text
.
├── scripts/
│   └── fetch_and_build.py        # データ収集・タグ付け・統計出力
├── docs/                         # GitHub Pages の公開ディレクトリ (Settings→Pages: main /docs)
│   ├── index.html
│   ├── app.js
│   ├── styles.css
│   └── data/
│       ├── citations.json        # 生成物: 論文リスト（タグ付き）
│       └── stats.json            # 生成物: 統計
├── data/
│   └── overrides.yml             # （任意）手動タグ上書き/除外設定
├── .github/
│   └── workflows/
│       └── update.yml            # スケジュール更新ワークフロー
└── README.md
```

---

## 2) 設定（最小）

* **GitHub Pages**: Repository → Settings → Pages → *Source*: **Deploy from a branch** / Branch: **main** / Folder: **/docs** に設定。
* **OpenAlex mailto**（任意）: OpenAlex API の利用統計に使用されます。メールは送信されません。

  * Repository → Settings → *Secrets and variables* → *Actions* → New repository secret で `OPENALEX_MAILTO` を作成（例: `you@example.com`）。
* （必要なら）`data/overrides.yml` に手動タグや除外を記述。

---

## 3) 自動タグの設計

**display\_name（タイトル）/ 概要 / venue 名**から正規表現ルールでタグを抽出します。初期ルールは以下（必要に応じて `scripts/fetch_and_build.py` 内 `TAG_RULES` を編集）。

* String Key / Text, Updatable, Disk, Main-memory, Multidimensional, Bloom Filter, Sketch, Hash Table, B-tree, LSM-tree, GPU, Distributed, Theoretical, Security/Adversarial, Compression, Benchmark, Range, Time-series, Disk-based Learned Index, **PGM-index**, **ALEX**, Learned Bloom Filter など。

> 手動上書きは `data/overrides.yml` で可能（ID 指定で add/remove）。

---

## 4) Python 収集・生成スクリプト

`scripts/fetch_and_build.py`

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Fetch all works that cite "The Case for Learned Index Structures" using OpenAlex,
assign auto tags, aggregate stats, and emit JSON files under docs/data/ for GitHub Pages.

- Source work is looked up by DOI, then we follow `cited_by_api_url` with cursor paging.
- No API key is required for OpenAlex; we include a `mailto` parameter if provided.
"""

from __future__ import annotations
import os, re, json, time, math
from collections import Counter, defaultdict
from pathlib import Path
from typing import Dict, Any, Iterable, List, Set

import requests

try:
    import yaml  # type: ignore
except Exception:
    yaml = None  # optional

ROOT = Path(__file__).resolve().parents[1]
DOCS = ROOT / "docs"
DATA_DIR = DOCS / "data"
DATA_DIR.mkdir(parents=True, exist_ok=True)

# ===== Configuration =====
# DOI of "The Case for Learned Index Structures" (SIGMOD'18)
TARGET_DOI = os.getenv("TARGET_DOI", "10.1145/3183713.3196909")
# Your email for OpenAlex usage statistics (optional, no emails sent)
OPENALEX_MAILTO = os.getenv("OPENALEX_MAILTO", os.getenv("OPENALEX_EMAIL", ""))
USER_AGENT = os.getenv("USER_AGENT", "learned-index-citations/1.0 (+GitHub Actions)")

# Tagging rules: (TAG_NAME, regex pattern). Case-insensitive search on title/abstract/venue.
TAG_RULES: List[tuple[str,str]] = [
    ("String Key", r"\b(string|text|varchar|character|lexicograph|dictionary)\b"),
    ("Updatable", r"\b(update|mutable|insert|delete|dynamic|online|incremental|lsm)\b"),
    ("Disk", r"\b(disk|ssd|storage|i/o|external memory|out[- ]of[- ]core)\b"),
    ("Main-memory", r"\b(in[- ]?memory|ram)\b"),
    ("Multidimensional", r"\b(multidimensional|multi[- ]?dimensional|spatial|kd[- ]?tree|r[- ]?tree|quadtree|octree)\b"),
    ("Bloom Filter", r"\b(bloom filter|learned bloom|\bLBF\b)\b"),
    ("Sketch", r"\b(count[- ]?min|cms|sketch|hyperloglog|countmin)\b"),
    ("Hash Table", r"\b(hash table|cuckoo|robin hood|tabulation|perfect hash)\b"),
    ("B-tree", r"\b(b[- ]?tree|b\+[- ]?tree|btree)\b"),
    ("LSM-tree", r"\b(lsm[- ]?tree|log[- ]?structured merge)\b"),
    ("GPU", r"\b(gpu|cuda)\b"),
    ("Distributed", r"\b(distributed|cluster|spark|hadoop|federated)\b"),
    ("Theoretical", r"\b(theorem|proof|approximation ratio|lower bound|upper bound|asymptotic|complexity)\b"),
    ("Security/Adversarial", r"\b(poison|adversarial|attack|robust|privacy|secure)\b"),
    ("Compression", r"\b(compress|compression|succinct|entropy)\b"),
    ("Benchmark", r"\b(benchmark|microbenchmark|sosd|workload|evaluation framework)\b"),
    ("Range", r"\b(range query|interval|scan)\b"),
    ("Time-series", r"\b(time[- ]?series|temporal)\b"),
    ("Disk-based Learned Index", r"learned index.*(disk|page|io|secondary storage)"),
    ("PGM-index", r"\bpgm[- ]?index\b"),
    ("ALEX", r"\balex\b"),
    ("Learned Bloom Filter", r"learned bloom filter|lbf"),
]

# ===== Helpers =====

def _session() -> requests.Session:
    s = requests.Session()
    s.headers.update({"User-Agent": USER_AGENT})
    return s


def get_work_by_doi(sess: requests.Session, doi: str) -> Dict[str, Any]:
    # Using the documented "external ID" syntax for DOIs
    url = f"https://api.openalex.org/works/https://doi.org/{doi}"
    params = {"mailto": OPENALEX_MAILTO} if OPENALEX_MAILTO else {}
    r = sess.get(url, params=params, timeout=30)
    r.raise_for_status()
    return r.json()


def iter_citations(sess: requests.Session, cited_by_api_url: str) -> Iterable[Dict[str, Any]]:
    """Iterate all citing works using OpenAlex cursor paging."""
    base = cited_by_api_url
    cursor = "*"
    per_page = 200  # max 200
    params = {"per-page": per_page, "cursor": cursor}
    if OPENALEX_MAILTO:
        params["mailto"] = OPENALEX_MAILTO
    # Reduce payload size via select
    params["select"] = ",".join([
        "id","display_name","publication_year","doi","cited_by_count",
        "host_venue","primary_location","authorships","concepts","abstract_inverted_index"
    ])

    total = 0
    while True:
        r = sess.get(base, params=params, timeout=60)
        r.raise_for_status()
        data = r.json()
        results = data.get("results", [])
        for w in results:
            yield w
        total += len(results)
        cursor = data.get("meta", {}).get("next_cursor")
        if not cursor:
            break
        params["cursor"] = cursor
        # polite delay
        time.sleep(0.2)


def text_blob(work: Dict[str,Any]) -> str:
    parts: List[str] = []
    title = work.get("display_name") or ""
    parts.append(str(title))
    # venue names
    for key in ("host_venue","primary_location"):
        hv = work.get(key) or {}
        if isinstance(hv, dict):
            dn = hv.get("display_name")
            if dn: parts.append(str(dn))
            src = hv.get("source") or {}
            if isinstance(src, dict):
                sdn = src.get("display_name")
                if sdn: parts.append(str(sdn))
    # abstract (inverted index → bag of words)
    inv = work.get("abstract_inverted_index") or {}
    if isinstance(inv, dict) and inv:
        parts.append(" ".join(inv.keys()))
    # concepts
    concepts = work.get("concepts") or []
    for c in concepts:
        dn = c.get("display_name")
        if dn: parts.append(str(dn))
    return " \n".join(parts).lower()


def auto_tags_for(work: Dict[str,Any]) -> List[str]:
    blob = text_blob(work)
    tags: Set[str] = set()
    for tag, pat in TAG_RULES:
        if re.search(pat, blob, flags=re.IGNORECASE):
            tags.add(tag)
    return sorted(tags)


def load_overrides() -> Dict[str, Any]:
    path = ROOT / "data" / "overrides.yml"
    if not path.exists() or yaml is None:
        return {}
    with open(path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f) or {}


def apply_overrides(items: List[Dict[str,Any]], overrides: Dict[str,Any]) -> None:
    add_map = {k: set(v or []) for k,v in (overrides.get("add_tags", {}) or {}).items()}
    remove_map = {k: set(v or []) for k,v in (overrides.get("remove_tags", {}) or {}).items()}
    hide_set = set(overrides.get("hide", []) or [])
    kept: List[Dict[str,Any]] = []
    for w in items:
        wid = w.get("id")
        if wid in hide_set:
            continue
        cur = set(w.get("tags", []))
        if wid in remove_map:
            cur -= remove_map[wid]
        if wid in add_map:
            cur |= add_map[wid]
        w["tags"] = sorted(cur)
        kept.append(w)
    items[:] = kept


def build_stats(items: List[Dict[str,Any]]) -> Dict[str,Any]:
    by_year = Counter([w.get("publication_year") for w in items if w.get("publication_year")])
    by_tag = Counter(tag for w in items for tag in w.get("tags", []))
    by_venue = Counter()
    citations_sum = 0
    author_counts = Counter()
    author_citations = Counter()

    for w in items:
        citations_sum += int(w.get("cited_by_count") or 0)
        hv = (w.get("host_venue") or {}).get("display_name")
        if hv: by_venue[hv] += 1
        for a in (w.get("authorships") or []):
            auth = a.get("author") or {}
            aid = auth.get("id") or auth.get("display_name") or "unknown"
            author_counts[aid] += 1
            author_citations[aid] += int(w.get("cited_by_count") or 0)

    top_authors = [
        {
            "author_id": aid,
            "name": aid.split("/")[-1] if aid.startswith("https://") else aid,
            "papers": cnt,
            "sum_citations": int(author_citations[aid]),
            "avg_citations": (author_citations[aid] / cnt) if cnt else 0.0,
        }
        for aid, cnt in author_counts.most_common(50)
    ]

    return {
        "total_works": len(items),
        "by_year": dict(sorted(by_year.items())),
        "by_tag": dict(sorted(by_tag.items(), key=lambda x:(-x[1], x[0]))),
        "by_venue": dict(by_venue.most_common(30)),
        "top_authors": top_authors,
        "citations_sum": citations_sum,
        "last_updated": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
    }


def main() -> None:
    sess = _session()
    work = get_work_by_doi(sess, TARGET_DOI)
    cited_by_url = work.get("cited_by_api_url")
    if not cited_by_url:
        raise SystemExit("cited_by_api_url not found in the work object.")

    items: List[Dict[str,Any]] = []
    for w in iter_citations(sess, cited_by_url):
        item = {
            "id": w.get("id"),
            "title": w.get("display_name"),
            "publication_year": w.get("publication_year"),
            "doi": w.get("doi"),
            "cited_by_count": w.get("cited_by_count"),
            "host_venue": (w.get("host_venue") or {}).get("display_name") or (w.get("primary_location") or {}).get("source",{}).get("display_name"),
            "landing_page_url": (w.get("primary_location") or {}).get("landing_page_url"),
            "authorships": [
                {
                    "author_id": (a.get("author") or {}).get("id"),
                    "name": (a.get("author") or {}).get("display_name"),
                    "institutions": [ (inst or {}).get("display_name") for inst in (a.get("institutions") or []) ],
                }
                for a in (w.get("authorships") or [])
            ],
        }
        item["tags"] = auto_tags_for(w)
        items.append(item)

    overrides = load_overrides()
    if overrides:
        apply_overrides(items, overrides)

    # Emit JSON
    citations_path = DATA_DIR / "citations.json"
    stats_path = DATA_DIR / "stats.json"

    with open(citations_path, "w", encoding="utf-8") as f:
        json.dump({"work": {
            "doi": TARGET_DOI,
            "openalex_id": work.get("id"),
            "display_name": work.get("display_name"),
            "cited_by_count": work.get("cited_by_count"),
        }, "results": items}, f, ensure_ascii=False, indent=2)

    stats = build_stats(items)
    with open(stats_path, "w", encoding="utf-8") as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)

    print(f"Wrote {citations_path}")
    print(f"Wrote {stats_path}")

if __name__ == "__main__":
    main()
```

---

## 5) GitHub Pages（フロントエンド）

`docs/index.html`

```html
<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Learned-Index Citations</title>
    <link rel="stylesheet" href="styles.css" />
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
  </head>
  <body>
    <header>
      <h1>Learned-Index Citations</h1>
      <p id="work-meta"></p>
    </header>

    <section id="controls">
      <input id="q" type="search" placeholder="Search title/author/venue…" />
      <select id="tag-filter" multiple></select>
      <button id="clear">Clear</button>
    </section>

    <section id="stats">
      <div class="stat"><div class="k" id="total_works">–</div><div class="v">papers</div></div>
      <div class="stat"><div class="k" id="citations_sum">–</div><div class="v">sum of cited_by_count</div></div>
      <div class="stat"><div class="k" id="last_updated">–</div><div class="v">last updated (UTC)</div></div>
    </section>

    <section id="charts">
      <canvas id="byYear"></canvas>
      <canvas id="byTag"></canvas>
    </section>

    <section>
      <h2>Top Authors (by #papers)</h2>
      <ol id="top-authors"></ol>
    </section>

    <section>
      <h2>Papers</h2>
      <div id="list" class="cards"></div>
    </section>

    <footer>
      <p>Built from OpenAlex data. This website is not affiliated with OpenAlex.</p>
      <p><a href="https://openalex.org/" target="_blank" rel="noopener">OpenAlex</a> • <a href="https://dl.acm.org/doi/10.1145/3183713.3196909" target="_blank" rel="noopener">RMI (DOI)</a></p>
    </footer>

    <script src="app.js"></script>
  </body>
</html>
```

`docs/app.js`

```javascript
async function loadJSON(path){
  const r = await fetch(path);
  if(!r.ok) throw new Error("Failed: "+path);
  return await r.json();
}

function el(tag, attrs={}, ...children){
  const e = document.createElement(tag);
  Object.entries(attrs).forEach(([k,v])=>{
    if(k === 'class') e.className = v; else if(k === 'html') e.innerHTML = v; else e.setAttribute(k, v);
  });
  for(const c of children){ if(typeof c === 'string') e.appendChild(document.createTextNode(c)); else if(c) e.appendChild(c); }
  return e;
}

function renderCard(w){
  const authors = (w.authorships||[]).map(a=>a.name).filter(Boolean).join(', ');
  const url = w.landing_page_url || (w.doi ? `https://doi.org/${w.doi.replace('https://doi.org/','')}` : null);
  const tags = (w.tags||[]).map(t=>el('span',{class:'tag'}, t));
  return el('article', {class:'card'},
    el('h3',{}, w.title||'(no title)'),
    el('div',{class:'meta'}, [authors || '(authors unknown)',' • ', w.host_venue||'(venue unknown)',' • ', w.publication_year||'–'].filter(Boolean).join(' ')),
    el('div',{class:'tags'}, ...tags),
    url ? el('a',{class:'btn', href:url, target:'_blank', rel:'noopener'}, 'Open') : null
  );
}

function filterPapers(papers, q, selectedTags){
  const k = (q||'').toLowerCase();
  return papers.filter(w=>{
    const hay = [w.title||'', w.host_venue||'', ...(w.authorships||[]).map(a=>a.name||'')].join('\n').toLowerCase();
    const okQ = !k || hay.includes(k);
    const okTag = !selectedTags.size || (w.tags||[]).some(t=>selectedTags.has(t));
    return okQ && okTag;
  });
}

function mountTagFilter(allTags){
  const sel = document.getElementById('tag-filter');
  sel.innerHTML = '';
  [...allTags].sort().forEach(t=>{
    sel.appendChild(el('option', {value:t}, t));
  });
  return sel;
}

function renderTopAuthors(stats){
  const ol = document.getElementById('top-authors');
  ol.innerHTML = '';
  for(const a of stats.top_authors||[]){
    const name = a.name;
    const li = el('li',{}, `${name}: ${a.papers} papers (avg cited_by ${a.avg_citations.toFixed(1)})`);
    ol.appendChild(li);
  }
}

function renderCounters(stats){
  document.getElementById('total_works').textContent = stats.total_works;
  document.getElementById('citations_sum').textContent = stats.citations_sum;
  document.getElementById('last_updated').textContent = stats.last_updated;
}

function renderCharts(stats){
  // Year chart
  const yc = document.getElementById('byYear').getContext('2d');
  const years = Object.keys(stats.by_year||{});
  const counts = Object.values(stats.by_year||{});
  new Chart(yc, {type:'bar', data:{labels:years, datasets:[{label:'papers / year', data:counts}]}});

  // Tag chart
  const tc = document.getElementById('byTag').getContext('2d');
  const tags = Object.keys(stats.by_tag||{}).slice(0,20);
  const vals = Object.values(stats.by_tag||{}).slice(0,20);
  new Chart(tc, {type:'bar', data:{labels:tags, datasets:[{label:'papers / tag (top20)', data:vals}]}});
}

(async function(){
  const citations = await loadJSON('data/citations.json');
  const stats = await loadJSON('data/stats.json');

  // Header meta
  const w = citations.work||{};
  const meta = document.getElementById('work-meta');
  meta.textContent = `${w.display_name||'Target work'} | OpenAlex: ${w.openalex_id||'N/A'} | cited_by_count: ${w.cited_by_count ?? 'N/A'}`;

  const papers = citations.results||[];
  const allTags = new Set(papers.flatMap(p=>p.tags||[]));
  renderCounters(stats);
  renderCharts(stats);
  renderTopAuthors(stats);

  const tagSel = mountTagFilter(allTags);
  const list = document.getElementById('list');
  const q = document.getElementById('q');
  const clear = document.getElementById('clear');

  function refresh(){
    const selected = new Set([...tagSel.selectedOptions].map(o=>o.value));
    const view = filterPapers(papers, q.value, selected);
    list.innerHTML = '';
    for(const w of view){ list.appendChild(renderCard(w)); }
  }

  q.addEventListener('input', refresh);
  tagSel.addEventListener('change', refresh);
  clear.addEventListener('click', ()=>{ q.value=''; [...tagSel.options].forEach(o=>o.selected=false); refresh(); });

  refresh();
})();
```

`docs/styles.css`

```css
:root{ --fg:#111; --bg:#fff; --muted:#6b7280; --card:#f9fafb; --accent:#2563eb; }
*{ box-sizing: border-box; }
html,body{ margin:0; padding:0; font:16px/1.5 system-ui, -apple-system, Segoe UI, Roboto, sans-serif; color:var(--fg); background:var(--bg); }
header{ padding:24px 16px; border-bottom:1px solid #eee; }
h1{ margin:0 0 4px; font-size:28px; }
#controls{ display:flex; gap:8px; padding:12px 16px; align-items:center; flex-wrap:wrap; border-bottom:1px solid #eee; }
#controls input[type=search]{ flex:1; min-width:240px; padding:8px 10px; border:1px solid #ddd; border-radius:8px; }
#controls select{ min-width:220px; height:96px; border:1px solid #ddd; border-radius:8px; padding:6px; }
#controls button{ padding:8px 12px; border:1px solid #ddd; border-radius:8px; background:#fff; cursor:pointer; }
#stats{ display:grid; grid-template-columns: repeat(auto-fit,minmax(160px,1fr)); gap:12px; padding:12px 16px; }
.stat{ background:var(--card); border:1px solid #eee; border-radius:12px; padding:12px; }
.stat .k{ font-size:24px; font-weight:700; }
.stat .v{ color:var(--muted); font-size:12px; }
#charts{ display:grid; gap:16px; padding:8px 16px 16px; grid-template-columns:1fr; }
.cards{ display:grid; grid-template-columns: repeat(auto-fit, minmax(280px, 1fr)); gap:12px; padding: 8px 16px 32px; }
.card{ background:#fff; border:1px solid #eee; border-radius:12px; padding:12px; display:flex; flex-direction:column; gap:8px; }
.card h3{ font-size:16px; margin:0; }
.card .meta{ color:var(--muted); font-size:12px; }
.tags{ display:flex; flex-wrap:wrap; gap:6px; }
.tag{ font-size:11px; border:1px solid #e5e7eb; padding:2px 6px; border-radius:999px; background:#f3f4f6; }
.btn{ display:inline-block; padding:6px 10px; border:1px solid var(--accent); color:var(--accent); border-radius:8px; text-decoration:none; width:max-content; }
.btn:hover{ background:rgba(37,99,235,.06); }
footer{ padding:16px; border-top:1px solid #eee; color:var(--muted); font-size:12px; }
```

---

## 6) 手動上書き（任意）

`data/overrides.yml`

```yaml
# Add / remove tags by OpenAlex work id
add_tags:
  "https://openalex.org/W1234567890": ["String Key", "Updatable"]
remove_tags:
  "https://openalex.org/W0987654321": ["Sketch"]
# Hide specific works from the site
hide:
  - "https://openalex.org/W0000000000"
```

---

## 7) GitHub Actions（自動更新）

`.github/workflows/update.yml`

```yaml
name: Update citations & build data

on:
  workflow_dispatch:
  schedule:
    - cron: "30 18 * * *"  # 03:30 JST daily (GitHub cron is UTC)

permissions:
  contents: write

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install requests pyyaml
      - name: Build data
        env:
          OPENALEX_MAILTO: ${{ secrets.OPENALEX_MAILTO }}
          TARGET_DOI: 10.1145/3183713.3196909
        run: |
          python scripts/fetch_and_build.py
      - name: Commit & push
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git add -A
          git commit -m "Update data: $(date -u +"%Y-%m-%dT%H:%MZ")" || echo "No changes"
          git push
```

---

## 8) README.md（抜粋）

````markdown
# Learned-Index Citations (GitHub Pages)

This site lists **all papers citing** “The Case for Learned Index Structures” (SIGMOD 2018 / DOI: 10.1145/3183713.3196909), with **auto tags** and **stats**. Data is refreshed daily via GitHub Actions from the OpenAlex API.

## How it works
1. `scripts/fetch_and_build.py` resolves the target work by DOI via `GET /works/https://doi.org/{doi}` and follows `cited_by_api_url` (cursor paging) to enumerate **all citing works**.
2. It assigns tags by simple regex heuristics (editable), applies optional `data/overrides.yml`, and writes `docs/data/*.json`.
3. `docs/index.html` (GitHub Pages) loads JSON and renders search, tag filter, charts (Chart.js), and lists.

## Configure
- Enable GitHub Pages from branch **main /docs**.
- (Optional) Add repository secret `OPENALEX_MAILTO` with your email for OpenAlex usage statistics.
- Optionally edit `data/overrides.yml` to add/remove tags or hide items.

## Local run
```bash
python -m venv .venv && source .venv/bin/activate
pip install requests pyyaml
export OPENALEX_MAILTO=you@example.com
python scripts/fetch_and_build.py
# Open docs/index.html in a browser (uses relative JSON paths)
````

## Notes

* OpenAlex API is free and no key is required. Respect rate limits; we include short delays and `mailto` for usage statistics.
* Citation counts shown for citing papers are `cited_by_count` reported by OpenAlex.
* This project is unaffiliated with OpenAlex/ACM.

```

---

## 9) 追加メモ
- **拡張**: Semantic Scholar API 連携（要 API key）で被引用のメタデータや PDF 链接を補強可能。/paper/{paperId}/citations を併用し、OpenAlex ID と DOI で突合せしてください。
- **タグ改善**: ルールベースに加え、キーワード辞書（例: ALEX, PGM, SOSD, learned bloom filter）や会議名（SIGMOD, VLDB, CIDR, VLDBJ, ICDE）での補助を追加。
- **パフォーマンス**: citing 件数が増えた場合でも cursor=200/req で十分高速です。初回のみ数千件規模でも数十秒程度で完了します（ネットワーク環境依存）。
- **JST 表示**: `stats.json` の UTC をフロント側で JST 表示に変換しても OK。

```
